# Advanced Web Crawler

A sophisticated web crawling and scraping application built with Next.js and Playwright.

## Features

- ğŸš€ **Advanced Browser Automation**: Powered by Playwright with stealth mode
- ğŸ” **Automatic Login Handling**: Detects and handles login forms
- ğŸ¤– **Human-like Behavior**: Random delays, mouse movements, and typing patterns
- ğŸ“Š **Real-time Progress Tracking**: Live updates during crawling
- ğŸ—ºï¸ **Visual Site Mapping**: Interactive visualization of discovered pages
- ğŸ“¸ **Screenshot Management**: Full-page screenshots of crawled pages
- ğŸ¯ **Smart Link Discovery**: Tracks CSS selectors and element relationships
- âš¡ **Rate Limiting**: Configurable delays to respect server resources
- ğŸ“ **JSON Export**: Comprehensive crawl results in structured format

## Quick Start

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Run the development server:**
   ```bash
   npm run dev
   ```

3. **Open your browser:**
   Navigate to [http://localhost:3000](http://localhost:3000)

## Usage

1. Enter the starting URL you want to crawl
2. Adjust the crawl depth (1-10 levels)
3. Configure advanced options if needed:
   - Login credentials for authenticated sites
   - Rate limiting settings
   - Domain restrictions
4. Click "Start Crawling" and monitor progress in real-time
5. Download results as JSON when complete

## Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/crawler/    # API routes for crawler operations
â”‚   â””â”€â”€ page.tsx        # Main UI page
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ crawler/        # Crawler-specific components
â”‚   â””â”€â”€ ui/            # Reusable UI components
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ crawler/       # Core crawler logic
â”‚   â”œâ”€â”€ types/         # TypeScript type definitions
â”‚   â””â”€â”€ utils.ts       # Utility functions
â””â”€â”€ crawl-results/     # Stored crawl results (auto-created)
```

## API Endpoints

- `POST /api/crawler/start` - Start a new crawl
- `GET /api/crawler/start?crawlId={id}` - Check crawl status

## Configuration

The crawler supports various configuration options:

```typescript
{
  startUrl: string;              // URL to start crawling from
  maxDepth: number;              // Maximum crawl depth (1-10)
  rateLimit: number;             // Requests per minute
  loginCredentials?: {           // Optional login credentials
    username: string;
    password: string;
  };
  domainRestrictions?: {         // Domain crawling rules
    stayWithinDomain: boolean;
    includeSubdomains: boolean;
  };
}
```

## Output Format

The crawler generates comprehensive JSON output including:

- Crawl metadata (duration, success rate, etc.)
- Site structure with navigation paths
- Link relationships with CSS selectors
- Page content and technical data
- Screenshots information
- Error logs

## Development

```bash
# Run development server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Run linting
npm run lint
```

## Requirements

- Node.js 18+ 
- Modern browser for UI
- Sufficient disk space for screenshots

## Notes

- Playwright browsers are automatically installed via postinstall script
- Screenshots are stored in the `screenshots/` directory
- Crawl results are saved in `crawl-results/` as JSON files
- The crawler respects robots.txt by default
- Rate limiting prevents overwhelming target servers

## Future Enhancements

- WebSocket support for real-time updates
- CAPTCHA solving integration
- Proxy rotation support
- Export to CSV/Excel formats
- Visual site map generation
- Docker containerization
